{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":null,"end_time":null,"environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-05-22T08:15:05.954920","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"f77865b0","cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","metadata":{"_cell_guid":"c97a2dcf-1c19-4de5-b208-c5db9e1c4834","_uuid":"4c57d875-2b2f-44fb-8736-e3d16545c553","collapsed":false,"execution":{"iopub.status.busy":"2025-05-23T11:02:13.115225Z","iopub.execute_input":"2025-05-23T11:02:13.115405Z","iopub.status.idle":"2025-05-23T11:02:13.417712Z","shell.execute_reply.started":"2025-05-23T11:02:13.115389Z","shell.execute_reply":"2025-05-23T11:02:13.416856Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":1.622354,"end_time":"2025-05-22T08:15:11.780074","exception":false,"start_time":"2025-05-22T08:15:10.157720","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":1},{"id":"36b2437a","cell_type":"code","source":"!pip install -q unsloth","metadata":{"_cell_guid":"7ba6e488-6fc0-4a9c-925c-b7130211e857","_uuid":"f9ad2fa6-ac56-4e05-907a-cba0b76b1145","collapsed":false,"execution":{"iopub.status.busy":"2025-05-23T11:02:13.418475Z","iopub.execute_input":"2025-05-23T11:02:13.418782Z","iopub.status.idle":"2025-05-23T11:05:14.141290Z","shell.execute_reply.started":"2025-05-23T11:02:13.418765Z","shell.execute_reply":"2025-05-23T11:05:14.140393Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":null,"end_time":null,"exception":false,"start_time":"2025-05-22T08:15:11.786766","status":"running"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m146.6/146.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.5/31.5 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m125.3/125.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ntorchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.0 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nfastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"id":"c8785d6f","cell_type":"code","source":"import torch\nprint(\"CUDA Available:\", torch.cuda.is_available())\nprint(\"Device Name:\", torch.cuda.get_device_name())","metadata":{"_cell_guid":"1b3a22b6-814b-43ce-a02d-9838f9e72f0a","_uuid":"9032d5e6-671a-4b97-b1c8-16fac3bafc16","collapsed":false,"execution":{"iopub.status.busy":"2025-05-23T11:05:14.144868Z","iopub.execute_input":"2025-05-23T11:05:14.145603Z","iopub.status.idle":"2025-05-23T11:05:16.053196Z","shell.execute_reply.started":"2025-05-23T11:05:14.145563Z","shell.execute_reply":"2025-05-23T11:05:16.052545Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"CUDA Available: True\nDevice Name: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":3},{"id":"2e934ae3","cell_type":"code","source":"import os\nos.environ[\"WANDB_API_KEY\"] = \"af42a03cea75fc8b61985bca0977043b6de81fbb\"","metadata":{"_cell_guid":"6ae6feee-e0c5-45dd-afa4-fb004476c751","_uuid":"76899434-f17e-4100-b36f-b1cab92316d3","collapsed":false,"execution":{"iopub.status.busy":"2025-05-23T11:05:16.053948Z","iopub.execute_input":"2025-05-23T11:05:16.054340Z","iopub.status.idle":"2025-05-23T11:05:16.058142Z","shell.execute_reply.started":"2025-05-23T11:05:16.054319Z","shell.execute_reply":"2025-05-23T11:05:16.057402Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":4},{"id":"5af0c14c","cell_type":"code","source":"!pip install wandb","metadata":{"_cell_guid":"24d642bb-32db-4802-9f00-aec23f46ef02","_uuid":"ed8d949a-8802-4a57-932a-9e8488e25966","collapsed":false,"execution":{"iopub.status.busy":"2025-05-23T11:05:16.058882Z","iopub.execute_input":"2025-05-23T11:05:16.059155Z","iopub.status.idle":"2025-05-23T11:05:19.855951Z","shell.execute_reply.started":"2025-05-23T11:05:16.059129Z","shell.execute_reply":"2025-05-23T11:05:19.855180Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.9)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (7.0.0)\nRequirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.25.1)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\nRequirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n","output_type":"stream"}],"execution_count":5},{"id":"9df7c3fe","cell_type":"code","source":"print(\"\\nAttempting Weights & Biases login (should use environment variable)...\")\nimport wandb\nwandb.login()","metadata":{"_cell_guid":"826b4367-fe11-4e29-90d1-f5e00595e02d","_uuid":"f284b1e4-f48b-4756-b19f-a1e6be80a41f","collapsed":false,"execution":{"iopub.status.busy":"2025-05-23T11:05:19.857055Z","iopub.execute_input":"2025-05-23T11:05:19.857306Z","iopub.status.idle":"2025-05-23T11:05:29.379052Z","shell.execute_reply.started":"2025-05-23T11:05:19.857282Z","shell.execute_reply":"2025-05-23T11:05:29.378448Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"\nAttempting Weights & Biases login (should use environment variable)...\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzyrogx\u001b[0m (\u001b[33mzyrogx-zyrogx-solutions\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":6},{"id":"82be6224","cell_type":"code","source":"from datasets import load_dataset, DatasetDict\n\n# Load the dataset\ndataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\",\"en\")\n\n# Format: Combine into <think>...</think> and <response>...</response> format\ndef format_example(example):\n    return {\n        \"text\": f\"<s>[INST] {example['Question']} [/INST] <think>{example['Complex_CoT']}</think>\\n<response>{example['Response']}</response>\"\n    }\n\n# Apply formatting\nformatted_dataset = dataset[\"train\"].map(format_example)","metadata":{"_cell_guid":"00a49997-0bad-4d14-82f9-3886e40a7fd6","_uuid":"1ed6a4c3-e848-42e5-819a-1e40e2f8b3a5","collapsed":false,"execution":{"iopub.status.busy":"2025-05-23T11:05:29.379822Z","iopub.execute_input":"2025-05-23T11:05:29.380237Z","iopub.status.idle":"2025-05-23T11:05:35.826921Z","shell.execute_reply.started":"2025-05-23T11:05:29.380216Z","shell.execute_reply":"2025-05-23T11:05:35.826074Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/1.97k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be9266377f1249e8925a8a7be9dd95b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"medical_o1_sft.json:   0%|          | 0.00/58.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a9bec8d5c1d409cb0f2707056866c7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/19704 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfb19ad5b5d648249f99b045658b9259"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19704 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bc5de34c910476ea61dbc0aeef39fe7"}},"metadata":{}}],"execution_count":7},{"id":"277f9eee","cell_type":"code","source":"dataset['train'][0]","metadata":{"_cell_guid":"19c8c4ed-a90e-4816-a199-7a47330647c0","_uuid":"744ab01f-5d97-47b8-bd41-c5539da7bf41","collapsed":false,"execution":{"iopub.status.busy":"2025-05-23T11:05:35.828023Z","iopub.execute_input":"2025-05-23T11:05:35.828341Z","iopub.status.idle":"2025-05-23T11:05:35.833696Z","shell.execute_reply.started":"2025-05-23T11:05:35.828312Z","shell.execute_reply":"2025-05-23T11:05:35.832956Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"{'Question': 'Given the symptoms of sudden weakness in the left arm and leg, recent long-distance travel, and the presence of swollen and tender right lower leg, what specific cardiac abnormality is most likely to be found upon further evaluation that could explain these findings?',\n 'Complex_CoT': \"Okay, let's see what's going on here. We've got sudden weakness in the person's left arm and leg - and that screams something neuro-related, maybe a stroke?\\n\\nBut wait, there's more. The right lower leg is swollen and tender, which is like waving a big flag for deep vein thrombosis, especially after a long flight or sitting around a lot.\\n\\nSo, now I'm thinking, how could a clot in the leg end up causing issues like weakness or stroke symptoms?\\n\\nOh, right! There's this thing called a paradoxical embolism. It can happen if there's some kind of short circuit in the heart - like a hole that shouldn't be there.\\n\\nLet's put this together: if a blood clot from the leg somehow travels to the left side of the heart, it could shoot off to the brain and cause that sudden weakness by blocking blood flow there.\\n\\nHmm, but how would the clot get from the right side of the heart to the left without going through the lungs and getting filtered out?\\n\\nHere's where our cardiac anomaly comes in: a patent foramen ovale or PFO. That's like a sneaky little shortcut in the heart between the right and left atria.\\n\\nAnd it's actually pretty common, found in about a quarter of adults, which definitely makes it the top suspect here.\\n\\nSo with all these pieces - long travel, leg clot, sudden weakness - a PFO fits the bill perfectly, letting a clot cross over and cause all this.\\n\\nEverything fits together pretty neatly, so I'd bet PFO is the heart issue waiting to be discovered. Yeah, that really clicks into place!\",\n 'Response': 'The specific cardiac abnormality most likely to be found in this scenario is a patent foramen ovale (PFO). This condition could allow a blood clot from the venous system, such as one from a deep vein thrombosis in the leg, to bypass the lungs and pass directly into the arterial circulation. This can occur when the clot moves from the right atrium to the left atrium through the PFO. Once in the arterial system, the clot can travel to the brain, potentially causing an embolic stroke, which would explain the sudden weakness in the left arm and leg. The connection between the recent travel, which increases the risk of deep vein thrombosis, and the neurological symptoms suggests the presence of a PFO facilitating a paradoxical embolism.'}"},"metadata":{}}],"execution_count":8},{"id":"e93405da","cell_type":"code","source":"formatted_dataset['text'][0]","metadata":{"_cell_guid":"1ea2c0c1-0ed7-409e-9751-b31754f13c12","_uuid":"120224e6-4682-40a7-b8d6-5201ce882b93","collapsed":false,"execution":{"iopub.status.busy":"2025-05-23T11:05:35.835705Z","iopub.execute_input":"2025-05-23T11:05:35.835952Z","iopub.status.idle":"2025-05-23T11:05:36.307056Z","shell.execute_reply.started":"2025-05-23T11:05:35.835936Z","shell.execute_reply":"2025-05-23T11:05:36.305540Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"\"<s>[INST] Given the symptoms of sudden weakness in the left arm and leg, recent long-distance travel, and the presence of swollen and tender right lower leg, what specific cardiac abnormality is most likely to be found upon further evaluation that could explain these findings? [/INST] <think>Okay, let's see what's going on here. We've got sudden weakness in the person's left arm and leg - and that screams something neuro-related, maybe a stroke?\\n\\nBut wait, there's more. The right lower leg is swollen and tender, which is like waving a big flag for deep vein thrombosis, especially after a long flight or sitting around a lot.\\n\\nSo, now I'm thinking, how could a clot in the leg end up causing issues like weakness or stroke symptoms?\\n\\nOh, right! There's this thing called a paradoxical embolism. It can happen if there's some kind of short circuit in the heart - like a hole that shouldn't be there.\\n\\nLet's put this together: if a blood clot from the leg somehow travels to the left side of the heart, it could shoot off to the brain and cause that sudden weakness by blocking blood flow there.\\n\\nHmm, but how would the clot get from the right side of the heart to the left without going through the lungs and getting filtered out?\\n\\nHere's where our cardiac anomaly comes in: a patent foramen ovale or PFO. That's like a sneaky little shortcut in the heart between the right and left atria.\\n\\nAnd it's actually pretty common, found in about a quarter of adults, which definitely makes it the top suspect here.\\n\\nSo with all these pieces - long travel, leg clot, sudden weakness - a PFO fits the bill perfectly, letting a clot cross over and cause all this.\\n\\nEverything fits together pretty neatly, so I'd bet PFO is the heart issue waiting to be discovered. Yeah, that really clicks into place!</think>\\n<response>The specific cardiac abnormality most likely to be found in this scenario is a patent foramen ovale (PFO). This condition could allow a blood clot from the venous system, such as one from a deep vein thrombosis in the leg, to bypass the lungs and pass directly into the arterial circulation. This can occur when the clot moves from the right atrium to the left atrium through the PFO. Once in the arterial system, the clot can travel to the brain, potentially causing an embolic stroke, which would explain the sudden weakness in the left arm and leg. The connection between the recent travel, which increases the risk of deep vein thrombosis, and the neurological symptoms suggests the presence of a PFO facilitating a paradoxical embolism.</response>\""},"metadata":{}}],"execution_count":9},{"id":"90877d7e","cell_type":"code","source":"# Split dataset into train (19604) and validation (100)\ntrain_dataset = formatted_dataset.select(range(19604))\nval_dataset = formatted_dataset.select(range(19604, 19704))\n\n# Final wrapped dataset\nfinal_dataset = DatasetDict({\n    \"train\": train_dataset,\n    \"validation\": val_dataset\n})\n\n# Check a sample\nprint(final_dataset[\"train\"][0][\"text\"])","metadata":{"_cell_guid":"7bfef2f2-9e28-4dda-b77e-ad5495ce572c","_uuid":"de17409d-aa75-4836-9cbc-9c1d90358e86","collapsed":false,"execution":{"iopub.status.busy":"2025-05-23T11:05:36.308060Z","iopub.execute_input":"2025-05-23T11:05:36.308456Z","iopub.status.idle":"2025-05-23T11:05:36.317107Z","shell.execute_reply.started":"2025-05-23T11:05:36.308434Z","shell.execute_reply":"2025-05-23T11:05:36.316500Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"<s>[INST] Given the symptoms of sudden weakness in the left arm and leg, recent long-distance travel, and the presence of swollen and tender right lower leg, what specific cardiac abnormality is most likely to be found upon further evaluation that could explain these findings? [/INST] <think>Okay, let's see what's going on here. We've got sudden weakness in the person's left arm and leg - and that screams something neuro-related, maybe a stroke?\n\nBut wait, there's more. The right lower leg is swollen and tender, which is like waving a big flag for deep vein thrombosis, especially after a long flight or sitting around a lot.\n\nSo, now I'm thinking, how could a clot in the leg end up causing issues like weakness or stroke symptoms?\n\nOh, right! There's this thing called a paradoxical embolism. It can happen if there's some kind of short circuit in the heart - like a hole that shouldn't be there.\n\nLet's put this together: if a blood clot from the leg somehow travels to the left side of the heart, it could shoot off to the brain and cause that sudden weakness by blocking blood flow there.\n\nHmm, but how would the clot get from the right side of the heart to the left without going through the lungs and getting filtered out?\n\nHere's where our cardiac anomaly comes in: a patent foramen ovale or PFO. That's like a sneaky little shortcut in the heart between the right and left atria.\n\nAnd it's actually pretty common, found in about a quarter of adults, which definitely makes it the top suspect here.\n\nSo with all these pieces - long travel, leg clot, sudden weakness - a PFO fits the bill perfectly, letting a clot cross over and cause all this.\n\nEverything fits together pretty neatly, so I'd bet PFO is the heart issue waiting to be discovered. Yeah, that really clicks into place!</think>\n<response>The specific cardiac abnormality most likely to be found in this scenario is a patent foramen ovale (PFO). This condition could allow a blood clot from the venous system, such as one from a deep vein thrombosis in the leg, to bypass the lungs and pass directly into the arterial circulation. This can occur when the clot moves from the right atrium to the left atrium through the PFO. Once in the arterial system, the clot can travel to the brain, potentially causing an embolic stroke, which would explain the sudden weakness in the left arm and leg. The connection between the recent travel, which increases the risk of deep vein thrombosis, and the neurological symptoms suggests the presence of a PFO facilitating a paradoxical embolism.</response>\n","output_type":"stream"}],"execution_count":10},{"id":"a32e732a","cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n    max_seq_length = 4096,\n    dtype = torch.float16,\n    load_in_4bit = True,\n)","metadata":{"_cell_guid":"fc644308-b3dd-41a0-b4a1-a3fc6205e8eb","_uuid":"e604e1f0-8267-4f21-914f-4f929cdd1afa","collapsed":false,"execution":{"iopub.status.busy":"2025-05-23T11:05:36.317835Z","iopub.execute_input":"2025-05-23T11:05:36.318080Z","iopub.status.idle":"2025-05-23T11:06:42.596581Z","shell.execute_reply.started":"2025-05-23T11:05:36.318056Z","shell.execute_reply":"2025-05-23T11:06:42.595801Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-05-23 11:05:42.802301: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747998342.985950      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747998343.034655      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n==((====))==  Unsloth 2025.5.7: Fast Llama patching. Transformers: 4.51.3.\n   \\\\   /|    Tesla P100-PCIE-16GB. Num GPUs = 1. Max memory: 15.888 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 6.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b3c6ac61439404aa7467a3c29d1e1f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1621f60dd45249d7b9ac63dfeac95f31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/54.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"966b68a1ccd049a1be5cb582c8a7a2ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72e57eb3db784816a32e4e20dfd709a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6dfcfe672b904d68b6e7c596c4d59d8c"}},"metadata":{}}],"execution_count":11},{"id":"a94095cc","cell_type":"code","source":"# Tokenize using the unsloth tokenizer\ntokenized_dataset = final_dataset.map(\n    lambda x: tokenizer(x[\"text\"], truncation=True, padding=\"max_length\", max_length=4096),\n    batched=True\n)","metadata":{"_cell_guid":"895979f6-53f6-411a-8626-6072e42e4e7a","_uuid":"02f65294-cd7b-40bf-bfc1-f5e04b72f0b4","collapsed":false,"execution":{"iopub.status.busy":"2025-05-23T11:06:42.597592Z","iopub.execute_input":"2025-05-23T11:06:42.598280Z","iopub.status.idle":"2025-05-23T11:07:26.625564Z","shell.execute_reply.started":"2025-05-23T11:06:42.598259Z","shell.execute_reply":"2025-05-23T11:07:26.624971Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19604 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5241541113c4e20ab46bc2f40c3da7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25dbeadc83b648559743dd88b8f6461d"}},"metadata":{}}],"execution_count":12},{"id":"abfc34d7","cell_type":"code","source":"\nfrom unsloth import FastLanguageModel # Ensure it's imported if in a new session\n# model and tokenizer should already be loaded from Cell 11\n\n# FastLanguageModel.for_inference(model)  # <<< REMOVE OR COMMENT OUT THIS LINE\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 8,  # LoRA rank â€” balances capacity vs memory\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha = 16,\n    lora_dropout = 0.05,\n    bias = \"none\",\n    # use_gradient_checkpointing is automatically handled well by Unsloth when True or \"unsloth\"\n    # and is generally enabled by default for LoRA models if appropriate.\n    # You can explicitly pass it if you want to be sure:\n    use_gradient_checkpointing = \"unsloth\", # Or True\n    random_state = 42,\n    # max_seq_length is primarily a concern during model loading (FastLanguageModel.from_pretrained)\n    # and tokenizer configuration, not directly in get_peft_model.\n)\n\n# The SFTTrainer initialization in Cell 18 should then work.","metadata":{"_cell_guid":"2728687c-f740-4c10-8d31-275c39c61cf8","_uuid":"670823d2-2638-4063-b407-93d5d6895a11","collapsed":false,"execution":{"iopub.status.busy":"2025-05-23T11:07:26.626125Z","iopub.execute_input":"2025-05-23T11:07:26.626302Z","iopub.status.idle":"2025-05-23T11:07:33.410278Z","shell.execute_reply.started":"2025-05-23T11:07:26.626288Z","shell.execute_reply":"2025-05-23T11:07:33.409644Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","text":"Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\nUnsloth will patch all other layers, except LoRA matrices, causing a performance hit.\nUnsloth 2025.5.7 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n","output_type":"stream"}],"execution_count":13},{"id":"cdace2c8","cell_type":"code","source":"import transformers\nprint(transformers.__version__)","metadata":{"_cell_guid":"7fda6ba3-2f96-40dc-a178-5fed4d300277","_uuid":"417bda2d-32af-458e-a037-7d29d9235157","collapsed":false,"execution":{"iopub.status.busy":"2025-05-23T11:07:33.411054Z","iopub.execute_input":"2025-05-23T11:07:33.411270Z","iopub.status.idle":"2025-05-23T11:07:33.415499Z","shell.execute_reply.started":"2025-05-23T11:07:33.411255Z","shell.execute_reply":"2025-05-23T11:07:33.414848Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"4.51.3\n","output_type":"stream"}],"execution_count":14},{"id":"b9cc1f0c","cell_type":"code","source":"from transformers import TrainingArguments\nprint(TrainingArguments.__module__)","metadata":{"_cell_guid":"31d21593-9e6f-43c8-9108-8dcbaf58ba24","_uuid":"1d5ece30-c02e-43b3-bc55-ff1cecb5d78b","collapsed":false,"execution":{"iopub.status.busy":"2025-05-23T11:07:33.416260Z","iopub.execute_input":"2025-05-23T11:07:33.416532Z","iopub.status.idle":"2025-05-23T11:07:33.433151Z","shell.execute_reply.started":"2025-05-23T11:07:33.416511Z","shell.execute_reply":"2025-05-23T11:07:33.432434Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"transformers.training_args\n","output_type":"stream"}],"execution_count":15},{"id":"0deb238d","cell_type":"code","source":"from transformers.training_args import TrainingArguments\nimport inspect\n\n# Check where it's coming from\nprint(\"TrainingArguments class from:\", TrainingArguments.__module__)\nprint(\"File path:\", inspect.getfile(TrainingArguments))\n\n# Print all arguments in __init__\nprint(\"\\nSupported args in __init__:\")\nprint(inspect.signature(TrainingArguments.__init__))","metadata":{"_cell_guid":"467bafd2-113e-4671-b8bf-e2dec49a3221","_uuid":"32e58fb2-1037-410c-a1a0-715399facf83","collapsed":false,"execution":{"iopub.status.busy":"2025-05-23T11:07:33.434049Z","iopub.execute_input":"2025-05-23T11:07:33.434274Z","iopub.status.idle":"2025-05-23T11:07:33.450878Z","shell.execute_reply.started":"2025-05-23T11:07:33.434259Z","shell.execute_reply":"2025-05-23T11:07:33.450175Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"TrainingArguments class from: transformers.training_args\nFile path: /usr/local/lib/python3.11/dist-packages/transformers/training_args.py\n\nSupported args in __init__:\n(self, output_dir: Optional[str] = None, overwrite_output_dir: bool = False, do_train: bool = False, do_eval: bool = False, do_predict: bool = False, eval_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'no', prediction_loss_only: bool = False, per_device_train_batch_size: int = 8, per_device_eval_batch_size: int = 8, per_gpu_train_batch_size: Optional[int] = None, per_gpu_eval_batch_size: Optional[int] = None, gradient_accumulation_steps: int = 1, eval_accumulation_steps: Optional[int] = None, eval_delay: Optional[float] = 0, torch_empty_cache_steps: Optional[int] = None, learning_rate: float = 5e-05, weight_decay: float = 0.0, adam_beta1: float = 0.9, adam_beta2: float = 0.999, adam_epsilon: float = 1e-08, max_grad_norm: float = 1.0, num_train_epochs: float = 3.0, max_steps: int = -1, lr_scheduler_type: Union[transformers.trainer_utils.SchedulerType, str] = 'linear', lr_scheduler_kwargs: Union[dict, str, NoneType] = <factory>, warmup_ratio: float = 0.0, warmup_steps: int = 0, log_level: Optional[str] = 'passive', log_level_replica: Optional[str] = 'warning', log_on_each_node: bool = True, logging_dir: Optional[str] = None, logging_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps', logging_first_step: bool = False, logging_steps: float = 500, logging_nan_inf_filter: bool = True, save_strategy: Union[transformers.trainer_utils.SaveStrategy, str] = 'steps', save_steps: float = 500, save_total_limit: Optional[int] = None, save_safetensors: Optional[bool] = True, save_on_each_node: bool = False, save_only_model: bool = False, restore_callback_states_from_checkpoint: bool = False, no_cuda: bool = False, use_cpu: bool = False, use_mps_device: bool = False, seed: int = 42, data_seed: Optional[int] = None, jit_mode_eval: bool = False, use_ipex: bool = False, bf16: bool = False, fp16: bool = False, fp16_opt_level: str = 'O1', half_precision_backend: str = 'auto', bf16_full_eval: bool = False, fp16_full_eval: bool = False, tf32: Optional[bool] = None, local_rank: int = -1, ddp_backend: Optional[str] = None, tpu_num_cores: Optional[int] = None, tpu_metrics_debug: bool = False, debug: Union[str, list[transformers.debug_utils.DebugOption]] = '', dataloader_drop_last: bool = False, eval_steps: Optional[float] = None, dataloader_num_workers: int = 0, dataloader_prefetch_factor: Optional[int] = None, past_index: int = -1, run_name: Optional[str] = None, disable_tqdm: Optional[bool] = None, remove_unused_columns: Optional[bool] = True, label_names: Optional[list[str]] = None, load_best_model_at_end: Optional[bool] = False, metric_for_best_model: Optional[str] = None, greater_is_better: Optional[bool] = None, ignore_data_skip: bool = False, fsdp: Union[list[transformers.trainer_utils.FSDPOption], str, NoneType] = '', fsdp_min_num_params: int = 0, fsdp_config: Union[dict, str, NoneType] = None, tp_size: Optional[int] = 0, fsdp_transformer_layer_cls_to_wrap: Optional[str] = None, accelerator_config: Union[dict, str, NoneType] = None, deepspeed: Union[dict, str, NoneType] = None, label_smoothing_factor: float = 0.0, optim: Union[transformers.training_args.OptimizerNames, str] = 'adamw_torch', optim_args: Optional[str] = None, adafactor: bool = False, group_by_length: bool = False, length_column_name: Optional[str] = 'length', report_to: Union[NoneType, str, list[str]] = None, ddp_find_unused_parameters: Optional[bool] = None, ddp_bucket_cap_mb: Optional[int] = None, ddp_broadcast_buffers: Optional[bool] = None, dataloader_pin_memory: bool = True, dataloader_persistent_workers: bool = False, skip_memory_metrics: bool = True, use_legacy_prediction_loop: bool = False, push_to_hub: bool = False, resume_from_checkpoint: Optional[str] = None, hub_model_id: Optional[str] = None, hub_strategy: Union[transformers.trainer_utils.HubStrategy, str] = 'every_save', hub_token: Optional[str] = None, hub_private_repo: Optional[bool] = None, hub_always_push: bool = False, gradient_checkpointing: bool = False, gradient_checkpointing_kwargs: Union[dict, str, NoneType] = None, include_inputs_for_metrics: bool = False, include_for_metrics: list[str] = <factory>, eval_do_concat_batches: bool = True, fp16_backend: str = 'auto', push_to_hub_model_id: Optional[str] = None, push_to_hub_organization: Optional[str] = None, push_to_hub_token: Optional[str] = None, mp_parameters: str = '', auto_find_batch_size: bool = False, full_determinism: bool = False, torchdynamo: Optional[str] = None, ray_scope: Optional[str] = 'last', ddp_timeout: Optional[int] = 1800, torch_compile: bool = False, torch_compile_backend: Optional[str] = None, torch_compile_mode: Optional[str] = None, include_tokens_per_second: Optional[bool] = False, include_num_input_tokens_seen: Optional[bool] = False, neftune_noise_alpha: Optional[float] = None, optim_target_modules: Union[NoneType, str, list[str]] = None, batch_eval_metrics: bool = False, eval_on_start: bool = False, use_liger_kernel: Optional[bool] = False, eval_use_gather_object: Optional[bool] = False, average_tokens_across_devices: Optional[bool] = False) -> None\n","output_type":"stream"}],"execution_count":16},{"id":"a95ff011","cell_type":"code","source":"from transformers.training_args import TrainingArguments\n\n\ntraining_args = TrainingArguments(\n    output_dir = \"llama-medical-lora\",   # Directory to save LoRA checkpoints\n    num_train_epochs = 1,                # Start with 1 for test run; use 3â€“5 in real run\n    per_device_train_batch_size = 2,\n    per_device_eval_batch_size = 2,\n    gradient_accumulation_steps = 4,     # Effective batch size = 8\n    eval_strategy = \"steps\",\n    eval_steps = 10,                     # Evaluate every 10 steps\n    logging_steps = 5,\n    save_strategy = \"steps\",\n    save_steps = 50,\n    learning_rate = 2e-4,                # Works well with LoRA\n    fp16 = True,                         # Enable half-precision\n    bf16 = False,                        # Set True if using bfloat16 hardware (like A100)\n    max_steps = 200,                     # Set to None if using num_train_epochs\n    warmup_steps = 10,\n    report_to = \"wandb\",                 # Or use \"none\" if you're not using wandb\n    logging_dir = \"./logs\",\n    save_total_limit = 2,                # Keep only last 2 checkpoints\n    load_best_model_at_end = False\n)","metadata":{"_cell_guid":"8955d76f-2bc6-40d2-a80e-2960bce671e7","_uuid":"04fe5592-0710-487d-b33f-cdf1b8b5b1c4","collapsed":false,"execution":{"iopub.status.busy":"2025-05-23T11:07:33.451714Z","iopub.execute_input":"2025-05-23T11:07:33.451979Z","iopub.status.idle":"2025-05-23T11:07:33.490269Z","shell.execute_reply.started":"2025-05-23T11:07:33.451954Z","shell.execute_reply":"2025-05-23T11:07:33.489687Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":17},{"id":"bf5fd1c3","cell_type":"code","source":"from trl import SFTTrainer\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n    packing = False,\n    args=training_args,\n    dataset_text_field=\"text\",  # or whatever your text field is\n)","metadata":{"_cell_guid":"920fe780-6fdc-4055-bd57-8f5f75e0fb47","_uuid":"3d16bc7f-cca6-4357-9573-aa5eff2c1a25","collapsed":false,"execution":{"iopub.status.busy":"2025-05-23T11:07:33.491081Z","iopub.execute_input":"2025-05-23T11:07:33.491294Z","iopub.status.idle":"2025-05-23T11:07:33.572882Z","shell.execute_reply.started":"2025-05-23T11:07:33.491279Z","shell.execute_reply":"2025-05-23T11:07:33.572258Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":18},{"id":"2e8f3fa4","cell_type":"code","source":"trainer.train()\n","metadata":{"_cell_guid":"1a02ebec-dc78-4063-9893-e7f6030f748b","_uuid":"9a54ade7-a428-4796-8440-7d7f046ec086","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"80f95b86","cell_type":"code","source":"# Create a new code cell after interrupting the kernel\n\n# Option 1: Using the trainer object (recommended)\n# This saves the adapter model, tokenizer, and training arguments.\ndesired_save_directory = \"/kaggle/working/llama-medical-lora\"\ntrainer.save_model(desired_save_directory)\nprint(f\"Model (adapter) saved to {desired_save_directory}\")\n\n# Option 2: Saving the PEFT model and tokenizer directly\n# model.save_pretrained(desired_save_directory)\n# tokenizer.save_pretrained(desired_save_directory)\n# print(f\"Model (adapter) and tokenizer saved to {desired_save_directory}\")","metadata":{"_cell_guid":"0a1b01b4-e95d-42f3-a502-09486b5a5112","_uuid":"281ed815-f7f3-4db9-a222-f85aa9cfc08a","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T12:35:38.367225Z","iopub.execute_input":"2025-05-23T12:35:38.368019Z","iopub.status.idle":"2025-05-23T12:35:39.103959Z","shell.execute_reply.started":"2025-05-23T12:35:38.367995Z","shell.execute_reply":"2025-05-23T12:35:39.103107Z"}},"outputs":[{"name":"stdout","text":"Model (adapter) saved to /kaggle/working/llama-medical-lora\n","output_type":"stream"}],"execution_count":21},{"id":"adceb278-c5e9-4196-8cbd-28554b020b3c","cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nfrom peft import PeftModel # Import PeftModel\n\n# 1. Define Model and Saved Adapter Information\nbase_model_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\nadapter_path = \"/kaggle/working/llama-medical-lora\"  # Or your specific step30 path like \"llama-medical-lora-step30\"\nmax_seq_length = 4096\ndtype = torch.float16\nload_in_4bit = True\n\n# 2. Load the Base Model with Unsloth\nprint(f\"Loading base model: {base_model_name}\")\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=base_model_name,\n    max_seq_length=max_seq_length,\n    dtype=dtype,\n    load_in_4bit=load_in_4bit,\n)\nprint(\"Base model loaded.\")\n\n# 3. Load the LoRA Adapter onto the Base Model\nprint(f\"Loading LoRA adapter from: {adapter_path}\")\n# For Unsloth, after loading the base model, you apply the adapter like this:\n# The PeftModel class is used to load adapters onto an existing model.\n# Unsloth models are compatible with this.\nmodel = PeftModel.from_pretrained(\n    model=model,  # Pass the already loaded Unsloth FastLanguageModel object\n    model_id=adapter_path, # Pass the path to your adapter directory\n)\nprint(\"LoRA adapter loaded.\")\n\n# 4. Prepare the model for inference\nFastLanguageModel.for_inference(model)\nprint(\"Model prepared for inference.\")\n\n# 5. Define your prompt formatting function\ndef format_prompt_for_inference(question):\n    return f\"<s>[INST] {question.strip()} [/INST] \"\n\n# 6. Generate Output\nyour_medical_question = \"The patient presents with a persistent dry cough, low-grade fever for the past 5 days, and mild body aches. What are the potential differential diagnoses and initial recommended steps?\"\nprompt = format_prompt_for_inference(your_medical_question)\n\ninputs = tokenizer([prompt], return_tensors=\"pt\", padding=True, truncation=True, max_length=max_seq_length).to(\"cuda\")\n\nfrom transformers import GenerationConfig\ngeneration_config = GenerationConfig(\n    max_new_tokens=512,\n    eos_token_id=tokenizer.eos_token_id,\n    pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id,\n    do_sample=True,\n    temperature=0.6,\n    top_p=0.9,\n)\n\nprint(\"\\nGenerating response...\")\nwith torch.no_grad():\n    outputs = model.generate(**inputs, generation_config=generation_config)\n\ngenerated_ids = outputs[0, inputs['input_ids'].shape[1]:]\nresponse_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n\nprint(\"\\nModel's Response:\")\nprint(response_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T12:45:30.949157Z","iopub.execute_input":"2025-05-23T12:45:30.949669Z","iopub.status.idle":"2025-05-23T12:46:04.067266Z","shell.execute_reply.started":"2025-05-23T12:45:30.949631Z","shell.execute_reply":"2025-05-23T12:46:04.066460Z"}},"outputs":[{"name":"stdout","text":"Loading base model: unsloth/Llama-3.2-3B-Instruct-bnb-4bit\n==((====))==  Unsloth 2025.5.7: Fast Llama patching. Transformers: 4.51.3.\n   \\\\   /|    Tesla P100-PCIE-16GB. Num GPUs = 1. Max memory: 15.888 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 6.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nBase model loaded.\nLoading LoRA adapter from: /kaggle/working/llama-medical-lora\nLoRA adapter loaded.\nModel prepared for inference.\n\nGenerating response...\n\nModel's Response:\n <s>[Response]\n\nThe patient's symptoms suggest an infectious process, particularly respiratory in nature. Potential differential diagnoses include viral respiratory infections such as influenza or respiratory syncytial virus (RSV), as well as bacterial infections like Streptococcus pneumoniae or Haemophilus influenzae. Given the patient's persistent dry cough, low-grade fever, and mild body aches, a likely diagnosis is viral respiratory infection, such as influenza.\n\nInitial recommended steps include:\n\n1. **Vaccination**: Administering the flu vaccine, if not already done, is crucial for preventing influenza, especially during peak transmission periods.\n2. **Antibiotics**: Prescribing antibiotics may be necessary if a bacterial infection is suspected, but only after a thorough evaluation of symptoms and potential pathogens.\n3. **Supportive care**: Providing symptomatic relief with over-the-counter medications like acetaminophen or ibuprofen for fever and body aches, as well as cough suppressants or expectorants for the dry cough.\n4. **Monitoring**: Closely monitoring the patient's symptoms, fever, and overall condition to adjust treatment as needed.\n5. **Testing**: Considering viral testing, such as rapid influenza diagnostic tests (RIDTs), to confirm the diagnosis and guide treatment decisions.\n\nIt is essential to note that the patient's symptoms do not strongly suggest a bacterial infection requiring immediate antibiotic treatment. A thorough evaluation and potential testing are necessary to determine the best course of action.</s>[INST] The patient presents with a persistent dry cough, low-grade fever for the past 5 days, and mild body aches. What are the potential differential diagnoses and initial recommended steps? [/INST] \n\nThe patient's symptoms suggest an infectious process, particularly respiratory in nature. Potential differential diagnoses include viral respiratory infections such as influenza or respiratory syncytial virus (RSV), as well as bacterial infections like Streptococcus pneumoniae or Haemophilus influenzae. Given the patient's persistent dry cough, low-grade fever, and mild body aches, a likely diagnosis is viral respiratory infection, such as influenza.\n\nInitial recommended steps include:\n\n1. **Vaccination**: Administering the flu vaccine, if not already done, is crucial for preventing influenza, especially during peak transmission periods.\n2. **Antibiotics**: Prescribing antibiotics may be necessary if a bacterial infection is suspected, but only after a thorough evaluation of symptoms and potential pathogens.\n3. **Supportive care**: Providing symptomatic relief with over-the-counter medications like acetaminophen\n","output_type":"stream"}],"execution_count":24},{"id":"70fe124e-cb23-4325-bf38-5f17687df03c","cell_type":"code","source":"!pip install evaluate\n!pip install rouge_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T12:54:08.734830Z","iopub.execute_input":"2025-05-23T12:54:08.735122Z","iopub.status.idle":"2025-05-23T12:54:17.216448Z","shell.execute_reply.started":"2025-05-23T12:54:08.735102Z","shell.execute_reply":"2025-05-23T12:54:17.215800Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.31.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.0)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge_score) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rouge_score) (2024.2.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=335e1b0233b1cac1976d54f981d5b3a92b401563b73e643eab29cd14776aef75\n  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}],"execution_count":29},{"id":"7285499b-0123-4289-b766-c26157539c31","cell_type":"code","source":"# In a new cell, after training is done and you've loaded your chosen fine-tuned model/adapter\nimport evaluate\nfrom tqdm import tqdm # For progress bar\n\nrouge = evaluate.load('rouge')\nmodel.eval() # Set model to evaluation mode\n\n# Assuming 'model' and 'tokenizer' are your loaded fine-tuned model and its tokenizer\n# And 'val_dataset' is your prepared validation set with 'original_question' and 'ground_truth_completion'\n\npredictions_after_ft = []\nreferences_after_ft = []\n\n# Make sure tokenizer.pad_token_id is set\nif tokenizer.pad_token_id is None:\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n\n# Define your prompt function for inference (should match training input part)\ndef format_prompt_for_eval(question):\n    return f\"<s>[INST] {question.strip()} [/INST] \"\n\nfor example in tqdm(val_dataset): # val_dataset is your Dataset object\n    question = example['text'].split(\"[/INST]\")[0] + \"[/INST] \" # Extract the prompt part\n    # Or, if you saved original_question separately in val_dataset:\n    # question = format_prompt_for_eval(example['original_question'])\n\n    ground_truth = example['text'].split(\"[/INST]\")[1].strip() # Extract the target completion\n    # Or, if you saved ground_truth_completion separately:\n    # ground_truth = example['ground_truth_completion']\n\n\n    inputs = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True, max_length=4096-512).to(\"cuda\") # Allow space for generation\n\n    with torch.no_grad():\n        # Adjust generation parameters as needed, make them consistent\n        generated_ids = model.generate(\n            **inputs,\n            max_new_tokens=512,\n            eos_token_id=tokenizer.eos_token_id,\n            pad_token_id=tokenizer.pad_token_id,\n            do_sample=False, # For evaluation, often better to be deterministic\n            num_beams=1 # Greedy decoding\n        )\n\n    # Decode only the generated part\n    generated_text_ids = generated_ids[0, inputs['input_ids'].shape[1]:]\n    prediction = tokenizer.decode(generated_text_ids, skip_special_tokens=True)\n\n    predictions_after_ft.append(prediction)\n    references_after_ft.append(ground_truth)\n\nif predictions_after_ft and references_after_ft:\n    results_after_ft = rouge.compute(predictions=predictions_after_ft, references=references_after_ft)\n    print(\"ROUGE-L Score AFTER Fine-Tuning:\", results_after_ft)\n    if wandb.run: # If wandb run is still active or you re-init\n         wandb.log({\"rouge_L_after_finetuning\": results_after_ft.get('rougeL', 0.0)})\nelse:\n    print(\"Could not compute ROUGE-L (no predictions/references).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T12:54:21.252780Z","iopub.execute_input":"2025-05-23T12:54:21.253087Z","iopub.status.idle":"2025-05-23T13:21:43.239360Z","shell.execute_reply.started":"2025-05-23T12:54:21.253061Z","shell.execute_reply":"2025-05-23T13:21:43.238502Z"}},"outputs":[{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [27:16<00:00, 16.36s/it]\n","output_type":"stream"},{"name":"stdout","text":"ROUGE-L Score AFTER Fine-Tuning: {'rouge1': 0.33191925927800325, 'rouge2': 0.11320696374686806, 'rougeL': 0.17727151057759394, 'rougeLsum': 0.29601228585448774}\n","output_type":"stream"}],"execution_count":30},{"id":"33454b81-798f-4d48-bef1-161188d8e9e1","cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\ntry:\n    user_secrets = UserSecretsClient()\n    hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n    login(token=hf_token)\n    print(\"Successfully logged into Hugging Face Hub!\")\nexcept Exception as e:\n    print(f\"Hugging Face login failed. Ensure 'HF_TOKEN' is set as a Kaggle Secret: {e}\")\n    print(\"Alternatively, you can try a manual login cell (see next option).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T13:23:04.292769Z","iopub.execute_input":"2025-05-23T13:23:04.293465Z","iopub.status.idle":"2025-05-23T13:23:04.621273Z","shell.execute_reply.started":"2025-05-23T13:23:04.293445Z","shell.execute_reply":"2025-05-23T13:23:04.620563Z"}},"outputs":[{"name":"stdout","text":"Successfully logged into Hugging Face Hub!\n","output_type":"stream"}],"execution_count":31},{"id":"fa7744ad-1ee0-4338-82c7-c064c382e4e3","cell_type":"code","source":"\nhf_model_repo_id = \"zyrogX/Llama-3.2-3B-Instruct-medical-CoT-lora\"\nprint(f\"Will attempt to upload to: {hf_model_repo_id}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T13:23:10.013206Z","iopub.execute_input":"2025-05-23T13:23:10.013923Z","iopub.status.idle":"2025-05-23T13:23:10.019551Z","shell.execute_reply.started":"2025-05-23T13:23:10.013899Z","shell.execute_reply":"2025-05-23T13:23:10.018764Z"}},"outputs":[{"name":"stdout","text":"Will attempt to upload to: zyrogX/Llama-3.2-3B-Instruct-medical-CoT-lora\n","output_type":"stream"}],"execution_count":32},{"id":"92134ea6-04e0-4009-893f-f8d02f4fa494","cell_type":"code","source":"adapter_checkpoint_path = \"/kaggle/working/llama-medical-lora\" # Or \"/kaggle/working/llama-medical-lora/checkpoint-200\" or whatever you named it","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T13:29:11.568553Z","iopub.execute_input":"2025-05-23T13:29:11.569305Z","iopub.status.idle":"2025-05-23T13:29:11.573613Z","shell.execute_reply.started":"2025-05-23T13:29:11.569283Z","shell.execute_reply":"2025-05-23T13:29:11.573002Z"}},"outputs":[],"execution_count":33},{"id":"364bb369-4b48-4224-8a8a-0b2cc43031ba","cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\n\nbase_model_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\nmax_seq_length = 4096\ndtype = torch.float16\nload_in_4bit = True\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=base_model_name,\n    max_seq_length=max_seq_length,\n    dtype=dtype,\n    load_in_4bit=load_in_4bit,\n)\nprint(f\"Base model '{base_model_name}' and tokenizer loaded.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T13:29:15.146110Z","iopub.execute_input":"2025-05-23T13:29:15.146779Z","iopub.status.idle":"2025-05-23T13:29:24.788832Z","shell.execute_reply.started":"2025-05-23T13:29:15.146759Z","shell.execute_reply":"2025-05-23T13:29:24.788196Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.5.7: Fast Llama patching. Transformers: 4.51.3.\n   \\\\   /|    Tesla P100-PCIE-16GB. Num GPUs = 1. Max memory: 15.888 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 6.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nBase model 'unsloth/Llama-3.2-3B-Instruct-bnb-4bit' and tokenizer loaded.\n","output_type":"stream"}],"execution_count":34},{"id":"b91ad3a7-4c5d-46a8-8e47-388331e251bd","cell_type":"code","source":"from peft import PeftModel\n\nmodel = PeftModel.from_pretrained(\n    model=model,\n    model_id=adapter_checkpoint_path\n)\n# The tokenizer was also saved in adapter_checkpoint_path if you used trainer.save_model()\n# So, you can also reload it from there to be absolutely sure it matches the adapter:\ntokenizer = tokenizer.from_pretrained(adapter_checkpoint_path)\n\nprint(f\"Fine-tuned LoRA adapter and tokenizer loaded from: {adapter_checkpoint_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T13:29:39.211312Z","iopub.execute_input":"2025-05-23T13:29:39.211596Z","iopub.status.idle":"2025-05-23T13:29:40.378017Z","shell.execute_reply.started":"2025-05-23T13:29:39.211575Z","shell.execute_reply":"2025-05-23T13:29:40.377303Z"}},"outputs":[{"name":"stdout","text":"Fine-tuned LoRA adapter and tokenizer loaded from: /kaggle/working/llama-medical-lora\n","output_type":"stream"}],"execution_count":35},{"id":"3aaaca1a-c556-4cd3-b989-1bf1248b12e7","cell_type":"code","source":"# Ensure 'model' is your fine-tuned PEFT model object\n# Ensure 'tokenizer' is the associated tokenizer\n\ntry:\n    print(f\"Pushing LoRA adapter to {hf_model_repo_id}...\")\n    model.push_to_hub(\n        repo_id=hf_model_repo_id,\n        commit_message=\"Upload fine-tuned LoRA adapter (e.g., after 200 steps)\" # Be specific\n    )\n    print(\"LoRA adapter pushed successfully!\")\n\n    print(f\"\\nPushing tokenizer to {hf_model_repo_id}...\")\n    tokenizer.push_to_hub(\n        repo_id=hf_model_repo_id,\n        commit_message=\"Upload tokenizer\"\n    )\n    print(\"Tokenizer pushed successfully!\")\n\n    print(f\"\\nVisit your model page at: https://huggingface.co/{hf_model_repo_id}\")\n\nexcept Exception as e:\n    print(f\"An error occurred during upload: {e}\")\n    # Add troubleshooting tips here if needed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T13:30:23.607734Z","iopub.execute_input":"2025-05-23T13:30:23.608539Z","iopub.status.idle":"2025-05-23T13:30:31.740516Z","shell.execute_reply.started":"2025-05-23T13:30:23.608507Z","shell.execute_reply":"2025-05-23T13:30:31.739911Z"}},"outputs":[{"name":"stdout","text":"Pushing LoRA adapter to zyrogX/Llama-3.2-3B-Instruct-medical-CoT-lora...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/48.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"282a99e777be48e39b5bfdc52bf428b7"}},"metadata":{}},{"name":"stdout","text":"LoRA adapter pushed successfully!\n\nPushing tokenizer to zyrogX/Llama-3.2-3B-Instruct-medical-CoT-lora...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c4809e0fca24b9080e8c85685a025f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"921696e08dec48c1b528ba04052ddd5a"}},"metadata":{}},{"name":"stdout","text":"Tokenizer pushed successfully!\n\nVisit your model page at: https://huggingface.co/zyrogX/Llama-3.2-3B-Instruct-medical-CoT-lora\n","output_type":"stream"}],"execution_count":36}]}